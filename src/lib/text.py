import re


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: –ø—Ä–∏–≤–æ–¥–∏—Ç —Ä–µ–≥–∏—Å—Ç—Ä, –∑–∞–º–µ–Ω—è–µ—Ç —ë –Ω–∞ –µ, —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã."""
    if not isinstance(text, str):
        raise TypeError("–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    
    if not text:
        return ""
    
    # –ó–∞–º–µ–Ω–∞ —ë –Ω–∞ –µ
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    if casefold:
        text = text.casefold()
    
    # –ó–∞–º–µ–Ω–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'[\t\r\n]+', ' ', text)
    
    # –£–±–∏—Ä–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def tokenize(text: str) -> list[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞)."""
    if not isinstance(text, str):
        raise TypeError("–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    
    if not text:
        return []
    
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/_ –∏ —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏)
    pattern = r'\b\w+(?:-\w+)*\b'
    tokens = re.findall(pattern, text)
    
    return tokens


def count_freq(tokens: list[str]) -> dict[str, int]:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤."""
    if not isinstance(tokens, list):
        raise TypeError("–¢–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
    
    freq_dict = {}
    for token in tokens:
        freq_dict[token] = freq_dict.get(token, 0) + 1
    
    return freq_dict


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤."""
    if not isinstance(freq, dict):
        raise TypeError("–ß–∞—Å—Ç–æ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
    if not isinstance(n, int) or n < 0:
        raise ValueError("n –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º")
    
    if not freq:
        return []
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    items = list(freq.items())
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ - –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    items.sort(key=lambda x: (-x[1], x[0]))
    
    return items[:n]

if __name__ == "__main__":

    print(normalize('–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t'))
    print(normalize('—ë–∂–∏–∫, –Å–ª–∫–∞'))
    print(normalize('Hello\r\nWorld'))
    print(normalize('  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  '))

    print(tokenize('–ø—Ä–∏–≤–µ—Ç –º–∏—Ä'))
    print(tokenize('hello,world!!!'))
    print(tokenize('–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ'))
    print(tokenize('2025 –≥–æ–¥'))
    print(tokenize('emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ'))

    tokence1 = count_freq(["a", "b", "a", "c", "b", "a"])
    print(tokence1)
    print(top_n(tokence1, 2))

    tokence2 = count_freq(["bb","aa","bb","aa","cc"])
    print(tokence2)
    print(top_n(tokence2, 2))


